---
title: "Homework_8_Text_mining_FNH"
author: "Frederik Normann Holm"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction and loaded packages
I have chosen to reproduce the code from the task in class and i will consider the sentiment in the case of Game of Thrones
For text mining, the packages "pdftools", "tidytext", "textdata" and "ggwordcloud" are loaded from the library
```{r get-packages}
library(tidyverse)
library(here)
library(pdftools)
library(tidytext)
library(textdata) 
library(ggwordcloud)
```

Next, i install the two different lexicons that i will be using in order to determine sentiment in the GOT pdf
```{r get-lexicons}
get_sentiments(lexicon = "nrc")
get_sentiments(lexicon = "afinn")
```
## Getting the GOT pdf file

Here i load the got.pdf into my project
```{r get-document}
got_path <- here("data","got.pdf")
got_text <- pdf_text(got_path)
```
In order to test if the code above was successfull, i will try to print a given page of the got.pdf 
```{r single-page}
got_p12 <- got_text[12]
got_p12
```
Now that it works, i will continue with wrangling

### Wrangling

I start by making each line on each page its own row
```{r split-lines}
got_df <- data.frame(got_text) %>% 
  mutate(text_full = str_split(got_text, pattern = '\\n')) %>% 
  unnest(text_full) %>% 
  mutate(text_full = str_trim(text_full)) #str_trim will trim starting and trailing spaces
```
Now i want to fetch individual words (tokens) in a tidy format by making use of an element from the 'tokenizer' package
```{r word-tokens}
got_tokens <- got_df %>% 
  unnest_tokens(word, text_full)
got_tokens
```
This differentiates from the previous chunk of code because each word has its own row now
now time for counting:
```{r counting-words}
got_wc <- got_tokens %>% 
  count(word) %>% 
  arrange(-n)
got_wc
```
From the word count above it is clear that there is a vast abundance of stop words that have to be filtered out in order to obtain a better overview of the sentiment:

### Removing stop words:

```{r stopwords}
got_stop <- got_tokens %>% 
  anti_join(stop_words) %>% 
  select(-got_text)
```
now checking the word count to see if the removal of stop words was successfull
```{r check-words}
got_swc <- got_stop %>% 
  count(word) %>% 
  arrange(-n)
got_swc
```
Now for numbers:
```{r numbers-removal}
got_no_numeric <- got_stop %>% 
  filter(is.na(as.numeric(word)))
```

### A word cloud of GOT words (non-numeric)
Now for some visualisation of the non-numeric words in Game of Thrones

```{r wordcloud-prep}
#since the book is over 700 pages in lenght, i only want to get a snippet of the words by sorting by top 100
got_top100 <- got_no_numeric %>% 
  count(word) %>% 
  arrange(-n) %>% 
  head(100)
```

```{r wordcloud}
ggplot(data = got_top100, aes(label = word, size = n)) +
  geom_text_wordcloud_area(aes(color = n), shape = "diamond") +
  scale_size_area(max_size = 14) +
  scale_color_gradientn(colors = c("darkgreen","orange","purple")) +
  theme_minimal()
```
## Sentiment analysis
First i load the 3 different lexicons i will be using for got.pdf in the same chunk
```{r loading-lexicons}
get_sentiments(lexicon = "afinn")
get_sentiments(lexicon = "bing")
get_sentiments(lexicon = "nrc")
```
Now its time for some sentiment analysis 

### With AFINN
First i bind the words from the got.pdf to the afinn lexicon
```{r bind-afinn}
got_afinn <- got_stop %>% 
  inner_join(get_sentiments("afinn")) #need to have a shared collumn with "inner_join"
```
Counting and plotting:
```{r count-plot}
got_afinn_hist <- got_afinn %>% 
  count(value)

ggplot(data = got_afinn_hist, aes(x = value, y = n)) +
  geom_col()
```
Delving deeper into the 3 words:
```{r afinn-3}
got_afinn3 <- got_afinn %>% 
  filter(value == 3)
got_afinn3
```
counting and plotting the words under the category "3" in positivity
```{r afinn-3-more}
unique(got_afinn3$word)

got_afinn3_n <- got_afinn3 %>% 
  count(word, sort = TRUE) %>% 
  mutate(word = fct_reorder(factor(word), n))


ggplot(data = got_afinn3_n, aes(x = word, y = n)) +
  geom_col() +
  coord_flip()

```
From my understanding, the words listed above can all be associated with "warm feelings"
Now i will summarize the afinn sentiment analysis:
```{r afinn-summary}
got_summary <- got_afinn %>% 
  summarize(
    mean_score = mean(value),
    median_score = median(value)
  )
got_summary
```
The mean and median hereby indicate *slightly* negative overall sentiments based on the AFINN lexicon.

### With NRC

We can use the NRC lexicon to start "binning" text by the feelings they're typically associated with. As above, we'll use inner_join() to combine the IPCC non-stopword text with the nrc lexicon: 
```{r bind-bing}
got_nrc <- got_stop %>% 
  inner_join(get_sentiments("nrc"))
```
Checking for excluded words:
```{r excluded-words}
got_exclude <- got_stop %>% 
  anti_join(get_sentiments("nrc"))

#Counting to find the most excluded:
got_exclude_n <- got_exclude %>% 
  count(word, sort = TRUE)

head(got_exclude_n)
```
Now that it is taken care of, some counting and plotting will commense:
```{r count-plot-bing}
got_nrc_n <- got_nrc %>% 
  count(sentiment, sort = TRUE)

#plotting:
ggplot(data = got_nrc_n, aes(x = sentiment, y = n)) +
  geom_col()

```
count by sentiment:
```{r count-nrc}
got_nrc_n5 <- got_nrc %>% 
  count(word,sentiment, sort = TRUE) %>% 
  group_by(sentiment) %>% 
  top_n(5) %>% 
  ungroup()

got_nrc_gg <- ggplot(data = got_nrc_n5, aes(x = reorder(word,n), y = n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, ncol = 2, scales = "free") +
  coord_flip() +
  theme_minimal() +
  labs(x = "Word", y = "count")

#plotting it
got_nrc_gg
```
It is interesting that the most frequent word "lord" is considered a negative word. This perhaps goes in tune with the case of "confidence" in the ipcc report 


